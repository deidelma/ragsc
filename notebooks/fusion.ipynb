{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fusion\n",
    "\n",
    "Attempt to apply the approach used by [Nir Diamant](https://github.com/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/fusion_retrieval.ipynb) to the ragsc problem.\n",
    "\n",
    "## Strategy\n",
    "\n",
    "Consider each cluster as a \"document\".  Using a random sample of the cluster data and associated embeddings, create a vector database\n",
    "using FAISS or Chroma.  At the same time, use Lucene to create an index for the \"documents\".  Score matches on both semantic (vector) and keyword (BM25) and combine the scores to see if we can get more success matching to clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# import libraries\n",
    "#\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from functools import partial, reduce\n",
    "from typing import Union\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# set constants\n",
    "#\n",
    "input_path = Path(\"../results\")\n",
    "output_path = Path(\"../results\")\n",
    "training_fraction = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total rows: 9370\n",
      "training set has 4685 rows\n",
      "test set has 4685 rows\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# load the data along with embeddings\n",
    "#\n",
    "master_df = pd.read_csv(input_path / Path(\"ragsc_00_all_large.csv\"))\n",
    "master_n_cells = master_df.shape[0]\n",
    "\n",
    "train_df = master_df.sample(frac=training_fraction)\n",
    "test_df= master_df.drop(train_df.index) #.sample(frac=training_fraction) \n",
    "print(f\"total rows: {master_df.shape[0]}\")\n",
    "print(f\"training set has {train_df.shape[0]} rows\")\n",
    "print(f\"test set has {test_df.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 624\n",
      "1 527\n",
      "2 380\n",
      "3 353\n",
      "4 367\n",
      "5 327\n",
      "6 276\n",
      "7 270\n",
      "8 240\n",
      "9 213\n",
      "10 201\n",
      "11 190\n",
      "12 147\n",
      "13 142\n",
      "14 107\n",
      "15 93\n",
      "16 102\n",
      "17 78\n",
      "18 48\n"
     ]
    }
   ],
   "source": [
    "for cluster in test_df.groupby('cluster'):\n",
    "    print(cluster[0], cluster[1].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gene_bags(df: pd.DataFrame, max_genes:int, sort_by_cluster_names=True) -> dict:\n",
    "    \"\"\"\n",
    "    Produces \"bags of words\" for each cluster to use as documents in BM25 analysis.\n",
    "    \n",
    "    Returns a dictionary with cluster name as the keys and a list of gene names as the values.\n",
    "    \"\"\"\n",
    "    clusters = df.groupby(\"cluster\", sort=False)\n",
    "    word_dict = {}\n",
    "    for cluster in clusters:\n",
    "        # each cluster is a tuple (cluster name, cluster dataframe)\n",
    "        words = []\n",
    "        cluster_df = cluster[1] # the dataframe\n",
    "        # convert each signature into a list of string\n",
    "        word_series = cluster_df.signature.apply(lambda x: x.split(\" \"))\n",
    "        # create a bag of words based containing the gene names for this cluster\n",
    "        for sig in word_series:\n",
    "            # retain only max_genes gene names to add to the bag of words\n",
    "            words.extend(sig[:max_genes]) \n",
    "        word_dict[cluster[0]] = words\n",
    "    if sort_by_cluster_names:\n",
    "        word_dict = {k: word_dict[k] for k in sorted(word_dict)}\n",
    "    return word_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# chunking\n",
    "#\n",
    "def chunk(s:Union[str,list], size:int, step=1) -> list[str]:\n",
    "    \"\"\"\n",
    "    Takes a string or list of strings and creates a list of overlapping chunks of a given size.\n",
    "\n",
    "    Args\n",
    "        size: The number of words (gene names) in each chunk.\n",
    "        step: The number of words to advance before the next chunk (defaults to 1).\n",
    "    Returns\n",
    "        A list of strings representing the chunks.\n",
    "    \"\"\"\n",
    "    if isinstance(s,str):\n",
    "        a = s.split()\n",
    "    else:\n",
    "        a = s\n",
    "    results = []\n",
    "    max = len(a)\n",
    "    for i in range(max):\n",
    "        if i+size < max:\n",
    "            results.append(\" \".join(a[i:i+size]))\n",
    "        else:\n",
    "            results.append(\" \".join(a[i:]))\n",
    "        i += step\n",
    "    return results\n",
    "    \n",
    "# chunks = chunk(\"this is a test of the splitter\", 3)\n",
    "# print(chunks)\n",
    "chunk2 = partial(chunk, size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunkn = partial(chunk, size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = get_gene_bags(train_df,max_genes=120)\n",
    "\n",
    "#\n",
    "# create index from the cluster \"documents\" which are stored in word_dict\n",
    "#\n",
    "docs = [chunk2(\" \".join(x)) for x in word_dict.values()]\n",
    "bm25_index = BM25Okapi(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(bm25, gene_list, max_genes=25, normalized=True) -> list[float]:\n",
    "    \"\"\"\n",
    "    Returns a list containing the scores for a particular list of genes\n",
    "    \"\"\"\n",
    "    query = chunk2(gene_list)[:max_genes]\n",
    "    scores = bm25.get_scores(query)\n",
    "    if normalized:\n",
    "        scores = (scores - np.min(scores))/(np.max(scores)-np.min(scores))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_score_column(df:pd.DataFrame, bm25, max_genes) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add a column to the provided dataframe containing the BM25 scores.\n",
    "\n",
    "    Args:\n",
    "        df - the dataframe whose signatures will be used to generate the scores\n",
    "        bm25 - the index to use fo comparison\n",
    "        max_genes - the maximum number of genes to include from each signature\n",
    "\n",
    "    Returns a reference to the original dataframe\n",
    "    \"\"\"\n",
    "    df['scores'] = df.signature.apply(lambda x: get_score(bm25, x, max_genes))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = create_score_column(test_df, bm25_index, 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [1.         0.91118707 0.71737601 0.63492394 0.66268237 0.49215485\n",
      " 0.32284543 0.42814737 0.27423234 0.42334994 0.16279759 0.\n",
      " 0.27388819 0.2822286  0.01755665 0.15366833 0.12148767 0.12874504\n",
      " 0.10166728] 1.0\n"
     ]
    }
   ],
   "source": [
    "n=121\n",
    "cluster = test_df.cluster.iloc[n]\n",
    "scores = test_df.scores.iloc[n]\n",
    "rating = scores[cluster]\n",
    "print(cluster, scores, rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0     1.00     0.97\n",
      "    1     1.00     0.80\n",
      "    2     0.71     0.69\n",
      "    3     1.00     0.91\n",
      "    4     1.00     0.33\n",
      "    5     1.00     0.20\n",
      "    6     1.00     0.65\n",
      "    7     0.97     0.49\n",
      "    8     1.00     0.23\n",
      "    9     1.00     0.27\n",
      "   10     1.00     0.12\n",
      "   11     1.00     0.52\n",
      "   12     1.00     0.19\n",
      "   13     1.00     0.24\n",
      "   14     1.00     0.25\n",
      "   15     1.00     0.12\n",
      "   16     0.45     0.41\n",
      "   17     0.25     0.16\n",
      "   18     0.37     0.20\n"
     ]
    }
   ],
   "source": [
    "row = 25\n",
    "clusters = df_test.groupby('cluster')\n",
    "for cluster in clusters:\n",
    "    # print(cluster[1].shape)\n",
    "    local_df = cluster[1]\n",
    "    no = cluster[0]\n",
    "    if no> 0:\n",
    "        bad = no-1\n",
    "    else:\n",
    "        bad = no+1\n",
    "    scores = local_df.scores.iloc[row]\n",
    "    # print(scores)\n",
    "    print(f\"{no:5} {scores[no]:8.2f} {scores[bad]:8.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4685\n",
      "\n",
      "Cluster: 00:    0.993 (624)\n",
      "Cluster: 01:    0.962 (527)\n",
      "Cluster: 02:    0.875 (380)\n",
      "Cluster: 03:    0.898 (353)\n",
      "Cluster: 04:    0.879 (367)\n",
      "Cluster: 05:    0.963 (327)\n",
      "Cluster: 06:    0.988 (276)\n",
      "Cluster: 07:    0.784 (270)\n",
      "Cluster: 08:    0.957 (240)\n",
      "Cluster: 09:    0.992 (213)\n",
      "Cluster: 10:    0.965 (201)\n",
      "Cluster: 11:    0.985 (190)\n",
      "Cluster: 12:    0.639 (147)\n",
      "Cluster: 13:    0.830 (142)\n",
      "Cluster: 14:    0.922 (107)\n",
      "Cluster: 15:    0.811 (93)\n",
      "Cluster: 16:    0.635 (102)\n",
      "Cluster: 17:    0.391 (78)\n",
      "Cluster: 18:    0.386 (48)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# explore the effect of sample size on mean score\n",
    "#\n",
    "print(test_df.shape[0])\n",
    "print()\n",
    "sum=0\n",
    "n_clusters = 19\n",
    "count=0\n",
    "# test_df['scores_sum'] = test_df.scores.apply(lambda x: np.sum(x))\n",
    "for cluster in test_df.groupby('cluster'):\n",
    "    cluster_no = cluster[0]\n",
    "    cluster_df = cluster[1]\n",
    "    cluster_df['predicted_score'] = cluster_df.scores.apply(lambda x: x[cluster_no])\n",
    "    # assume score is normalized\n",
    "    avg_score_for_cluster = cluster_df.predicted_score.sum() / cluster_df.shape[0]\n",
    "    print(f\"Cluster: {cluster_no:02}: {avg_score_for_cluster:8.3f} ({cluster_df.shape[0]:02})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: 00:(608)\n",
      "Cluster: 01:(547)\n",
      "Cluster: 02:(411)\n",
      "Cluster: 03:(374)\n",
      "Cluster: 04:(328)\n",
      "Cluster: 05:(288)\n",
      "Cluster: 06:(295)\n",
      "Cluster: 07:(271)\n",
      "Cluster: 08:(251)\n",
      "Cluster: 09:(193)\n",
      "Cluster: 10:(197)\n",
      "Cluster: 11:(178)\n",
      "Cluster: 12:(167)\n",
      "Cluster: 13:(151)\n",
      "Cluster: 14:(113)\n",
      "Cluster: 15:(108)\n",
      "Cluster: 16:(97)\n",
      "Cluster: 17:(70)\n",
      "Cluster: 18:(38)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# explore the effect of training set size on mean score\n",
    "#\n",
    "for cluster in train_df.groupby('cluster'):\n",
    "    cluster_no = cluster[0]\n",
    "    cluster_df = cluster[1]\n",
    "    print(f\"Cluster: {cluster_no:02}:({cluster_df.shape[0]:02})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# save intermediate results\n",
    "#\n",
    "train_df.to_csv(\"data/train.csv\")\n",
    "test_df.to_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector database strategy\n",
    "\n",
    "Each cluster is a text document.\n",
    "Each cell signature is a sentence.\n",
    "Need to chunk the cluster documents and restrict sentences to the highest expression genes. A reasonable cut point is 120 genes based on the BM25 analysis, which showed plateauing in the matches at around this number of \"words\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "562200\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# docs contains the chunked gene names by cluster\n",
    "#\n",
    "\n",
    "total = reduce(lambda x,y: x+y, [len(x) for x in docs],0)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# given that current form of docs is prohibitively large (n=562200 chunks),\n",
    "# will use current embeddings as a first attempt\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_embeddings(\n",
    "    collection: chromadb.Collection,\n",
    "    df: pd.DataFrame,\n",
    "    min_item=0,\n",
    "    max_item=-1,\n",
    "    embeddings_column: str = \"embeddings\",\n",
    "    docs_column: str = \"cluster\",\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Stores embeddings in the provided ChromaDB collection.\n",
    "\n",
    "    Args\n",
    "    collection: the collection to receive the data\n",
    "    df : the dataframe from which the data is derived\n",
    "    min_item: the minimum row number to use\n",
    "    max_item: the maximum row number to use, defaults to -1 (all rows)\n",
    "    embeddings_column: the column containing the embeddings, defaults to \"embeddings\"\n",
    "    docs_column: the column containing the document name, defaults to \"cluster\"\n",
    "\n",
    "    Rerturns the number of embeddings added to the database\n",
    "    \"\"\"\n",
    "    if max_item == -1:\n",
    "        max_item = df.shape[0]\n",
    "    if max_item <= min_item:\n",
    "        logger.error(\"max_item must be greater than min_item\")\n",
    "        return 0\n",
    "    docs = [] # clusters\n",
    "    embeds = [] # embeddings\n",
    "    ids = [] # cell ids\n",
    "    for i in range(min_item, max_item):\n",
    "        docs.append(str(df[docs_column].iloc[i]))\n",
    "        embeds.append(json.loads(df[embeddings_column].iloc[i]))\n",
    "        ids.append(str(df.index[i]))\n",
    "    try:\n",
    "        collection.add(documents=docs, embeddings=embeds, ids=ids)\n",
    "    except Exception as e:\n",
    "        logger.error(\"unable to load data into database\")\n",
    "        logger.exception(e)\n",
    "        return 0\n",
    "    else:\n",
    "        return max_item - min_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_database(collection_name: str = \"ragsc\") -> chromadb.Collection:\n",
    "    client = chromadb.Client()\n",
    "    try:\n",
    "        c = client.get_collection(collection_name)\n",
    "        client.delete_collection(collection_name)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    c = client.create_collection(collection_name)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_database(df: pd.DataFrame) -> chromadb.Collection:\n",
    "    \"\"\"\n",
    "    creates an in memory ChromaDB collection  based on the data in the \n",
    "    provided dataframe.\n",
    "    \"\"\"\n",
    "    collection = initialize_database()\n",
    "    df = df[~df.signature.isnull()]  # clean any empty signatures\n",
    "    store_embeddings(collection, df)\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_embeddings(embeddings:str, collection:chromadb.Collection, n_results=100):\n",
    "    results = collection.query(\n",
    "        query_embeddings=[json.loads(embeddings)],\n",
    "        n_results=n_results,\n",
    "        include=[\"documents\",\"distances\"])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# need to test results based on cluster orientation\n",
    "#\n",
    "def test_item(df: pd.DataFrame, row: int, collection: chromadb.Collection, n_results=100):\n",
    "    # print(f\"Original cluster: {df.cluster.iloc[row]}\")\n",
    "    results = collection.query(\n",
    "        query_embeddings=[json.loads(df.embeddings.iloc[row])],\n",
    "        n_results=n_results,\n",
    "        include=[\"documents\",\"distances\"]\n",
    "    )\n",
    "    # print(results)\n",
    "    # return zip(results['documents'],results['distances'])\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# first attempt\n",
    "#\n",
    "coll = setup_database(train_df)\n",
    "results = test_item(test_df,0,coll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_score(data: list[float], offset = 0.01)->float:\n",
    "    if len(data) == 0:\n",
    "        return 0 \n",
    "    a = np.array(data)\n",
    "    a = np.log10(1.0/(a+offset))\n",
    "    return a.sum()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_score_per_row(embeddings:str, coll, n_results=100, max_clusters=19):\n",
    "    results = test_embeddings(embeddings,coll)\n",
    "    pairs = list(zip(results['documents'][0],results['distances'][0]))\n",
    "    table ={}\n",
    "    for k in range(max_cluster): \n",
    "        table[k] = []\n",
    "    for item in pairs:\n",
    "        table[int(item[0])].append(item[1])\n",
    "    scores = []\n",
    "    for k in table:\n",
    "        scores.append(distance_score(table[k]))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_distance_score(df: pd.DataFrame, coll: chromadb.Collection, n_results=100) -> None:\n",
    "    \"\"\"\n",
    "    Calculates distance_score on a row-wise basis, storing the results\n",
    "    in column \"d_score\" and normalized (x-min/max-min) in \"n_score\".\n",
    "    \n",
    "    This works in-place, modifying the input dataframe by adding two columns.\n",
    "    \"\"\"\n",
    "    # first create columns of lists to store the results\n",
    "    df['d_score'] = [[] for i in range(df.shape[0])]\n",
    "    df['n_score'] = [[] for i in range(df.shape[0])]\n",
    "\n",
    "    # now calculate the scores and normalized scores\n",
    "    df.d_score = df.embeddings.apply(lambda x: distance_score_per_row(x, coll))\n",
    "    df.n_score = df.d_score.apply(lambda x: (x-np.min(x))/(np.max(x) - np.min(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# calculate the distance scores\n",
    "#\n",
    "apply_distance_score(test_df, coll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# calculate the combined index\n",
    "#\n",
    "alpha = 0 # alpha is the proportion to assign to each score\n",
    "\n",
    "df_test['overall'] = df_test.scores * alpha + (1 - alpha) * df_test.n_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: 00:    0.757 (624)\n",
      "Cluster: 01:    0.998 (527)\n",
      "Cluster: 02:    0.626 (380)\n",
      "Cluster: 03:    0.672 (353)\n",
      "Cluster: 04:    0.614 (367)\n",
      "Cluster: 05:    0.702 (327)\n",
      "Cluster: 06:    0.997 (276)\n",
      "Cluster: 07:    0.427 (270)\n",
      "Cluster: 08:    0.757 (240)\n",
      "Cluster: 09:    0.948 (213)\n",
      "Cluster: 10:    0.847 (201)\n",
      "Cluster: 11:    0.903 (190)\n",
      "Cluster: 12:    0.209 (147)\n",
      "Cluster: 13:    0.687 (142)\n",
      "Cluster: 14:    0.989 (107)\n",
      "Cluster: 15:    0.749 (93)\n",
      "Cluster: 16:    0.441 (102)\n",
      "Cluster: 17:    0.192 (78)\n",
      "Cluster: 18:    0.147 (48)\n"
     ]
    }
   ],
   "source": [
    "clusters = df_test.groupby('cluster')\n",
    "for cluster in clusters:\n",
    "    cluster_no = cluster[0]\n",
    "    cluster_df = cluster[1]\n",
    "    cluster_df['accuracy_score'] = cluster_df.overall.apply(lambda x: x[cluster_no])\n",
    "    # assume score is normalized\n",
    "    avg_score_for_cluster = cluster_df.accuracy_score.sum() / cluster_df.shape[0]\n",
    "    print(f\"Cluster: {cluster_no:02}: {avg_score_for_cluster:8.3f} ({cluster_df.shape[0]:02})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "BM = 0\n",
    "VECTOR = 1\n",
    "BOTH = 2\n",
    "def calculate_summary_stats(clusters_df: pd.DataFrame, method:int) -> dict[int,np.array]:\n",
    "    clusters = clusters_df.groupby('cluster')\n",
    "    table = {k:[] for k in range(df_test.cluster.max())}\n",
    "    for cluster in clusters:\n",
    "        cluster_no = cluster[0]\n",
    "        cluster_df = cluster[1]\n",
    "        row_count = cluster_df.shape[0]\n",
    "        values = np.zeros(row_count,dtype=float)\n",
    "        for row in range(row_count):\n",
    "            n_score = cluster_df.n_score.iloc[row][cluster_no]\n",
    "            m_score = cluster_df.scores.iloc[row][cluster_no]\n",
    "            if method == BM:\n",
    "                values[row] = m_score\n",
    "            elif method == VECTOR:\n",
    "                values[row] = n_score\n",
    "            else:\n",
    "                if n_score > m_score:\n",
    "                    values[row] = n_score\n",
    "                else:\n",
    "                    values[row] = m_score\n",
    "        table[cluster_no] = values\n",
    "    return table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00    0.757    0.013 (624)00    0.993    0.001 (624)00    0.996    0.001 (624)\n",
      "01    0.998    0.002 (527)01    0.962    0.003 (527)01    1.000    0.000 (527)\n",
      "02    0.626    0.016 (380)02    0.875    0.007 (380)02    0.916    0.006 (380)\n",
      "03    0.672    0.017 (353)03    0.898    0.007 (353)03    0.938    0.006 (353)\n",
      "04    0.614    0.015 (367)04    0.879    0.007 (367)04    0.897    0.007 (367)\n",
      "05    0.702    0.019 (327)05    0.963    0.005 (327)05    0.975    0.004 (327)\n",
      "06    0.997    0.002 (276)06    0.988    0.003 (276)06    1.000    0.000 (276)\n",
      "07    0.427    0.018 (270)07    0.784    0.010 (270)07    0.823    0.010 (270)\n",
      "08    0.757    0.019 (240)08    0.957    0.007 (240)08    0.971    0.006 (240)\n",
      "09    0.948    0.012 (213)09    0.992    0.003 (213)09    0.994    0.002 (213)\n",
      "10    0.847    0.017 (201)10    0.965    0.007 (201)10    0.984    0.004 (201)\n",
      "11    0.903    0.018 (190)11    0.985    0.006 (190)11    0.989    0.005 (190)\n",
      "12    0.209    0.011 (147)12    0.639    0.013 (147)12    0.642    0.014 (147)\n",
      "13    0.687    0.024 (142)13    0.830    0.017 (142)13    0.875    0.014 (142)\n",
      "14    0.989    0.008 (107)14    0.922    0.012 (107)14    0.998    0.002 (107)\n",
      "15    0.749    0.031 (93)15    0.811    0.021 (93)15    0.885    0.019 (93)\n",
      "16    0.441    0.030 (102)16    0.635    0.025 (102)16    0.688    0.026 (102)\n",
      "17    0.192    0.014 (78)17    0.391    0.020 (78)17    0.408    0.019 (78)\n",
      "18    0.147    0.017 (48)18    0.386    0.036 (48)18    0.395    0.035 (48)\n",
      "------------\n",
      "00   0.2\n",
      "01   3.9\n",
      "02   4.7\n",
      "03   4.5\n",
      "04   2.0\n",
      "05   1.2\n",
      "06   1.2\n",
      "07   5.0\n",
      "08   1.5\n",
      "09   0.2\n",
      "10   2.0\n",
      "11   0.5\n",
      "12   0.5\n",
      "13   5.4\n",
      "14   8.3\n",
      "15   9.0\n",
      "16   8.2\n",
      "17   4.3\n",
      "18   2.5\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "table_vector = calculate_summary_stats(df_test, VECTOR)\n",
    "table_match = calculate_summary_stats(df_test, BM)\n",
    "table_both = calculate_summary_stats(df_test, BOTH)\n",
    "for cluster in table_vector:\n",
    "    print(f\"{cluster:02} {table_vector[cluster].mean():8.3f} {stats.sem(table_vector[cluster]):8.3f} ({table_vector[cluster].size})\", end='')\n",
    "    print(f\"{cluster:02} {table_match[cluster].mean():8.3f} {stats.sem(table_match[cluster]):8.3f} ({table_match[cluster].size})\", end='')\n",
    "    print(f\"{cluster:02} {table_both[cluster].mean():8.3f} {stats.sem(table_both[cluster]):8.3f} ({table_both[cluster].size})\")\n",
    "print(\"--\" * 6)\n",
    "for cluster in table_vector:\n",
    "    print(f\"{cluster:02} {((table_both[cluster].mean()/table_match[cluster].mean())-1) * 100:5.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
