{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fusion\n",
    "\n",
    "Attempt to apply the approach used by [Nir Diamant](https://github.com/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/fusion_retrieval.ipynb) to the ragsc problem.\n",
    "\n",
    "## Strategy\n",
    "\n",
    "Consider each cluster as a \"document\".  Using a random sample of the cluster data and associated embeddings, create a vector database\n",
    "using FAISS or Chroma.  At the same time, use Lucene to create an index for the \"documents\".  Score matches on both semantic (vector) and keyword (BM25) and combine the scores to see if we can get more success matching to clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# import libraries\n",
    "#\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# set constants\n",
    "#\n",
    "input_path = Path(\"../results\")\n",
    "output_path = Path(\"../results\")\n",
    "training_fraction = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total rows: 9370\n",
      "training set has 4685 rows\n",
      "test set has 4685 rows\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# load the data along with embeddings\n",
    "#\n",
    "master_df = pd.read_csv(input_path / Path(\"ragsc_00_all_large.csv\"))\n",
    "master_n_cells = master_df.shape[0]\n",
    "\n",
    "train_df = master_df.sample(frac=training_fraction)\n",
    "test_df= master_df.drop(train_df.index) #.sample(frac=training_fraction) \n",
    "print(f\"total rows: {master_df.shape[0]}\")\n",
    "print(f\"training set has {train_df.shape[0]} rows\")\n",
    "print(f\"test set has {test_df.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 587\n",
      "1 547\n",
      "2 411\n",
      "3 364\n",
      "4 332\n",
      "5 303\n",
      "6 286\n",
      "7 276\n",
      "8 247\n",
      "9 212\n",
      "10 187\n",
      "11 176\n",
      "12 160\n",
      "13 155\n",
      "14 118\n",
      "15 97\n",
      "16 97\n",
      "17 84\n",
      "18 46\n"
     ]
    }
   ],
   "source": [
    "for cluster in test_df.groupby('cluster'):\n",
    "    print(cluster[0], cluster[1].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gene_bags(df: pd.DataFrame, max_genes:int, sort_by_cluster_names=True) -> dict:\n",
    "    \"\"\"\n",
    "    Produces \"bags of words\" for each cluster to use as documents in BM25 analysis.\n",
    "    \n",
    "    Returns a dictionary with cluster name as the keys and a list of gene names as the values.\n",
    "    \"\"\"\n",
    "    clusters = df.groupby(\"cluster\", sort=False)\n",
    "    word_dict = {}\n",
    "    for cluster in clusters:\n",
    "        # each cluster is a tuple (cluster name, cluster dataframe)\n",
    "        words = []\n",
    "        cluster_df = cluster[1] # the dataframe\n",
    "        # convert each signature into a list of string\n",
    "        word_series = cluster_df.signature.apply(lambda x: x.split(\" \"))\n",
    "        # create a bag of words based containing the gene names for this cluster\n",
    "        for sig in word_series:\n",
    "            # retain only max_genes gene names to add to the bag of words\n",
    "            words.extend(sig[:max_genes]) \n",
    "        word_dict[cluster[0]] = words\n",
    "    if sort_by_cluster_names:\n",
    "        word_dict = {k: word_dict[k] for k in sorted(word_dict)}\n",
    "    return word_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# create index from the cluster \"documents\" which are stored in word_dict\n",
    "#\n",
    "# bm25_index = BM25Okapi(word_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# chunking\n",
    "#\n",
    "def chunk(s:Union[str,list], size:int, step=1) -> list[str]:\n",
    "    \"\"\"\n",
    "    Takes a string or list of strings and creates a list of chunks of a given size.\n",
    "    \"\"\"\n",
    "    if isinstance(s,str):\n",
    "        a = s.split()\n",
    "    else:\n",
    "        a = s\n",
    "    results = []\n",
    "    max = len(a)\n",
    "    for i in range(max):\n",
    "        if i+size < max:\n",
    "            results.append(\" \".join(a[i:i+size]))\n",
    "        else:\n",
    "            results.append(\" \".join(a[i:]))\n",
    "        i += step\n",
    "    return results\n",
    "    \n",
    "# chunks = chunk(\"this is a test of the splitter\", 3)\n",
    "# print(chunks)\n",
    "chunk2 = partial(chunk, size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunkn = partial(chunk, size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = get_gene_bags(train_df,max_genes=120)\n",
    "\n",
    "#\n",
    "# create index from the cluster \"documents\" which are stored in word_dict\n",
    "#\n",
    "docs = [chunk2(\" \".join(x)) for x in word_dict.values()]\n",
    "bm25_index = BM25Okapi(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(bm25, gene_list, max_genes=25, normalized=True) -> list[float]:\n",
    "    \"\"\"\n",
    "    Returns a list containing the scores for a particular list of genes\n",
    "    \"\"\"\n",
    "    query = chunk2(gene_list)[:max_genes]\n",
    "    scores = bm25.get_scores(query)\n",
    "    if normalized:\n",
    "        scores = (scores - np.min(scores))/(np.max(scores)-np.min(scores))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_score_column(df:pd.DataFrame, bm25, max_genes) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add a column to the provided dataframe containing the BM25 scores.\n",
    "\n",
    "    Args:\n",
    "        df - the dataframe whose signatures will be used to generate the scores\n",
    "        bm25 - the index to use fo comparison\n",
    "        max_genes - the maximum number of genes to include from each signature\n",
    "\n",
    "    Returns a reference to the original dataframe\n",
    "    \"\"\"\n",
    "    df['scores'] = df.signature.apply(lambda x: get_score(bm25, x, max_genes))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = create_score_column(test_df, bm25_index, 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [1.         0.51558109 0.86709428 0.53305399 0.58166585 0.47954012\n",
      " 0.23371676 0.31831665 0.36897762 0.46709019 0.28887357 0.\n",
      " 0.29918718 0.24894921 0.05789937 0.22079566 0.22473557 0.00306533\n",
      " 0.03085111] 1.0\n"
     ]
    }
   ],
   "source": [
    "n=121\n",
    "cluster = test_df.cluster.iloc[n]\n",
    "scores = test_df.scores.iloc[n]\n",
    "rating = scores[cluster]\n",
    "print(cluster, scores, rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0     1.00     0.74\n",
      "    1     0.91     0.74\n",
      "    2     0.99     0.92\n",
      "    3     1.00     0.65\n",
      "    4     1.00     0.44\n",
      "    5     1.00     0.19\n",
      "    6     1.00     0.67\n",
      "    7     1.00     0.47\n",
      "    8     1.00     0.33\n",
      "    9     1.00     0.21\n",
      "   10     1.00     0.08\n",
      "   11     1.00     0.37\n",
      "   12     0.60     0.15\n",
      "   13     1.00     0.17\n",
      "   14     1.00     0.26\n",
      "   15     0.92     0.13\n",
      "   16     0.62     0.30\n",
      "   17     0.17     0.39\n",
      "   18     0.71     0.06\n"
     ]
    }
   ],
   "source": [
    "row = 25\n",
    "clusters = df_test.groupby('cluster')\n",
    "for cluster in clusters:\n",
    "    # print(cluster[1].shape)\n",
    "    local_df = cluster[1]\n",
    "    no = cluster[0]\n",
    "    if no> 0:\n",
    "        bad = no-1\n",
    "    else:\n",
    "        bad = no+1\n",
    "    scores = local_df.scores.iloc[row]\n",
    "    # print(scores)\n",
    "    print(f\"{no:5} {scores[no]:8.2f} {scores[bad]:8.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4685\n",
      "\n",
      "Cluster: 00:    0.996 (587)\n",
      "Cluster: 01:    0.956 (547)\n",
      "Cluster: 02:    0.847 (411)\n",
      "Cluster: 03:    0.880 (364)\n",
      "Cluster: 04:    0.913 (332)\n",
      "Cluster: 05:    0.967 (303)\n",
      "Cluster: 06:    0.985 (286)\n",
      "Cluster: 07:    0.755 (276)\n",
      "Cluster: 08:    0.953 (247)\n",
      "Cluster: 09:    0.993 (212)\n",
      "Cluster: 10:    0.970 (187)\n",
      "Cluster: 11:    0.994 (176)\n",
      "Cluster: 12:    0.587 (160)\n",
      "Cluster: 13:    0.796 (155)\n",
      "Cluster: 14:    0.882 (118)\n",
      "Cluster: 15:    0.840 (97)\n",
      "Cluster: 16:    0.651 (97)\n",
      "Cluster: 17:    0.382 (84)\n",
      "Cluster: 18:    0.359 (46)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# explore the effect of sample size on mean score\n",
    "#\n",
    "print(test_df.shape[0])\n",
    "print()\n",
    "sum=0\n",
    "n_clusters = 19\n",
    "count=0\n",
    "# test_df['scores_sum'] = test_df.scores.apply(lambda x: np.sum(x))\n",
    "for cluster in test_df.groupby('cluster'):\n",
    "    cluster_no = cluster[0]\n",
    "    cluster_df = cluster[1]\n",
    "    cluster_df['predicted_score'] = cluster_df.scores.apply(lambda x: x[cluster_no])\n",
    "    # assume score is normalized\n",
    "    avg_score_for_cluster = cluster_df.predicted_score.sum() / cluster_df.shape[0]\n",
    "    print(f\"Cluster: {cluster_no:02}: {avg_score_for_cluster:8.3f} ({cluster_df.shape[0]:02})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: 00:(645)\n",
      "Cluster: 01:(527)\n",
      "Cluster: 02:(380)\n",
      "Cluster: 03:(363)\n",
      "Cluster: 04:(363)\n",
      "Cluster: 05:(312)\n",
      "Cluster: 06:(285)\n",
      "Cluster: 07:(265)\n",
      "Cluster: 08:(244)\n",
      "Cluster: 09:(194)\n",
      "Cluster: 10:(211)\n",
      "Cluster: 11:(192)\n",
      "Cluster: 12:(154)\n",
      "Cluster: 13:(138)\n",
      "Cluster: 14:(102)\n",
      "Cluster: 15:(104)\n",
      "Cluster: 16:(102)\n",
      "Cluster: 17:(64)\n",
      "Cluster: 18:(40)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# explore the effect of training set size on mean score\n",
    "#\n",
    "for cluster in train_df.groupby('cluster'):\n",
    "    cluster_no = cluster[0]\n",
    "    cluster_df = cluster[1]\n",
    "    print(f\"Cluster: {cluster_no:02}:({cluster_df.shape[0]:02})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
