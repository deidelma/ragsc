{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fusion\n",
    "\n",
    "Attempt to apply the approach used by [Nir Diamant](https://github.com/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/fusion_retrieval.ipynb) to the ragsc problem.\n",
    "\n",
    "## Strategy\n",
    "\n",
    "Consider each cluster as a \"document\".  Using a random sample of the cluster data and associated embeddings, create a vector database\n",
    "using FAISS or Chroma.  At the same time, use Lucene to create an index for the \"documents\".  Score matches on both semantic (vector) and keyword (BM25) and combine the scores to see if we can get more success matching to clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# import libraries\n",
    "#\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# set constants\n",
    "#\n",
    "input_path = Path(\"../results\")\n",
    "output_path = Path(\"../results\")\n",
    "training_fraction = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# load the data along with embeddings\n",
    "#\n",
    "master_df = pd.read_csv(input_path / Path(\"ragsc_00_all_large.csv\"))\n",
    "master_n_cells = master_df.shape[0]\n",
    "\n",
    "train_df = master_df.sample(frac=2*training_fraction)\n",
    "test_df= master_df.drop(train_df.index).sample(frac=training_fraction) \n",
    "print(f\"training set has {train_df.shape[0]} rows\")\n",
    "print(f\"test set has {test_df.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# need to create \"documents\" based on clusters\n",
    "#\n",
    "clusters = train_df.groupby(\"cluster\", sort=False)\n",
    "\n",
    "word_dict = {}\n",
    "for cluster in clusters:\n",
    "    words = []\n",
    "    cluster_df = cluster[1]\n",
    "    word_series = cluster_df.signature.apply(lambda x: x.split(\" \"))\n",
    "    for sig in word_series:\n",
    "        words.extend(sig[:120])\n",
    "    word_dict[cluster[0]] = words\n",
    "#\n",
    "# sort by the keys (i.e., the clusters)\n",
    "#\n",
    "word_dict = {k: word_dict[k] for k in sorted(word_dict)}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# create index from the cluster \"documents\" which are stored in word_dict\n",
    "#\n",
    "bm25_index = BM25Okapi(word_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# let's start with a simple query\n",
    "#\n",
    "n=150\n",
    "q_sig = test_df.signature.iloc[n].split(\" \")\n",
    "cluster = test_df.cluster.iloc[n]\n",
    "\n",
    "\n",
    "#\n",
    "# let's get a score\n",
    "#\n",
    "bm25_scores = bm25_index.get_scores(q_sig[:100])\n",
    "bm25_scores = (bm25_scores - np.min(bm25_scores))/(np.max(bm25_scores)-np.min(bm25_scores))\n",
    "print(bm25_scores)\n",
    "print(\"-----\\n\",cluster, bm25_scores[cluster])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# chunking\n",
    "#\n",
    "def chunk(s:str, size:int, step=1) -> list[str]:\n",
    "    a = s.split()\n",
    "    results = []\n",
    "    max = len(a)\n",
    "    for i in range(max):\n",
    "        if i+size < max:\n",
    "            results.append(\" \".join(a[i:i+size]))\n",
    "        else:\n",
    "            results.append(\" \".join(a[i:]))\n",
    "        i += step\n",
    "    return results\n",
    "    \n",
    "chunks = chunk(\"this is a test of the splitter\", 3)\n",
    "print(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunkn = partial(chunk, size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"this is a test\",\n",
    "    \"oddly I have no hamburgers\",\n",
    "    \"wish I were here\"\n",
    "]\n",
    "\n",
    "docs = list(map(chunkn, docs))\n",
    "print(docs)\n",
    "\n",
    "bm_crap = BM25Okapi(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = chunkn(\"bob is a big boy now\")\n",
    "scores = bm_crap.get_scores(query)\n",
    "# scores = (scores - np.min(scores))/(np.max(scores)-np.min(scores))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# create index from the cluster \"documents\" which are stored in word_dict\n",
    "#\n",
    "# bm25_index = BM25Okapi(word_dict.values())\n",
    "docs = [chunkn(\" \".join(x)) for x in word_dict.values()]\n",
    "bm25_index = BM25Okapi(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=20\n",
    "query = chunkn(test_df.signature.iloc[n])[:25]\n",
    "cluster = test_df.cluster.iloc[n]\n",
    "\n",
    "scores = bm25_index.get_scores(query)\n",
    "print(len(query))\n",
    "scores = (scores - np.min(scores))/(np.max(scores)-np.min(scores))\n",
    "print(cluster,scores, scores[cluster])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_df.shape[0])\n",
    "sum=0\n",
    "max = test_df.shape[0]\n",
    "count=0\n",
    "for i in range(test_df.shape[0]):\n",
    "    query = chunkn(test_df.signature.iloc[i])[:100]\n",
    "    cluster = test_df.cluster.iloc[i]\n",
    "    scores = bm25_index.get_scores(query)\n",
    "    if np.max(scores) > np.min(scores):\n",
    "        scores = (scores - np.min(scores))/(np.max(scores)-np.min(scores))\n",
    "        # print(cluster, scores[cluster])\n",
    "        sum += scores[cluster]\n",
    "        count += 1\n",
    "    # else:\n",
    "        # print(cluster,-1)\n",
    "print(f\"Sum:{sum} Percent:{sum/max*100.0}, Count:{count}, Zeros:{max-count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
