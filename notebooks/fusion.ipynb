{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fusion\n",
    "\n",
    "Attempt to apply the approach used by [Nir Diamant](https://github.com/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/fusion_retrieval.ipynb) to the ragsc problem.\n",
    "\n",
    "## Strategy\n",
    "\n",
    "Consider each cluster as a \"document\".  Using a random sample of the cluster data and associated embeddings, create a vector database\n",
    "using FAISS or Chroma.  At the same time, use Lucene to create an index for the \"documents\".  Score matches on both semantic (vector) and keyword (BM25) and combine the scores to see if we can get more success matching to clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# import libraries\n",
    "#\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# set constants\n",
    "#\n",
    "input_path = Path(\"../results\")\n",
    "output_path = Path(\"../results\")\n",
    "training_fraction = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set has 3748 rows\n",
      "test set has 1124 rows\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# load the data along with embeddings\n",
    "#\n",
    "master_df = pd.read_csv(input_path / Path(\"ragsc_00_all_large.csv\"))\n",
    "master_n_cells = master_df.shape[0]\n",
    "\n",
    "train_df = master_df.sample(frac=2*training_fraction)\n",
    "test_df= master_df.drop(train_df.index).sample(frac=training_fraction) \n",
    "print(f\"training set has {train_df.shape[0]} rows\")\n",
    "print(f\"test set has {test_df.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# need to create \"documents\" based on clusters\n",
    "#\n",
    "clusters = train_df.groupby(\"cluster\", sort=False)\n",
    "\n",
    "word_dict = {}\n",
    "for cluster in clusters:\n",
    "    words = []\n",
    "    cluster_df = cluster[1]\n",
    "    word_series = cluster_df.signature.apply(lambda x: x.split(\" \"))\n",
    "    for sig in word_series:\n",
    "        words.extend(sig[:120])\n",
    "    word_dict[cluster[0]] = words\n",
    "#\n",
    "# sort by the keys (i.e., the clusters)\n",
    "#\n",
    "word_dict = {k: word_dict[k] for k in sorted(word_dict)}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# create index from the cluster \"documents\" which are stored in word_dict\n",
    "#\n",
    "bm25_index = BM25Okapi(word_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.86725646 0.83802347 1.         0.87957711 0.64294957 0.37730684\n",
      " 0.06037929 0.66324782 0.15628418 0.15227508 0.05380354 0.09892914\n",
      " 0.44558922 0.13895247 0.         0.00835163 0.0319021  0.03352001\n",
      " 0.11006756]\n",
      "-----\n",
      " 1 0.8380234748505551\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# let's start with a simple query\n",
    "#\n",
    "n=150\n",
    "q_sig = test_df.signature.iloc[n].split(\" \")\n",
    "cluster = test_df.cluster.iloc[n]\n",
    "\n",
    "\n",
    "#\n",
    "# let's get a score\n",
    "#\n",
    "bm25_scores = bm25_index.get_scores(q_sig[:100])\n",
    "bm25_scores = (bm25_scores - np.min(bm25_scores))/(np.max(bm25_scores)-np.min(bm25_scores))\n",
    "print(bm25_scores)\n",
    "print(\"-----\\n\",cluster, bm25_scores[cluster])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this is a', 'is a test', 'a test of', 'test of the', 'of the splitter', 'the splitter', 'splitter']\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# chunking\n",
    "#\n",
    "def chunk(s:str, size:int, step=1) -> list[str]:\n",
    "    a = s.split()\n",
    "    results = []\n",
    "    max = len(a)\n",
    "    for i in range(max):\n",
    "        if i+size < max:\n",
    "            results.append(\" \".join(a[i:i+size]))\n",
    "        else:\n",
    "            results.append(\" \".join(a[i:]))\n",
    "        i += step\n",
    "    return results\n",
    "    \n",
    "chunks = chunk(\"this is a test of the splitter\", 3)\n",
    "print(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunkn = partial(chunk, size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this is', 'is a', 'a test', 'test'], ['oddly I', 'I have', 'have no', 'no hamburgers', 'hamburgers'], ['wish I', 'I were', 'were here', 'here']]\n"
     ]
    }
   ],
   "source": [
    "docs = [\n",
    "    \"this is a test\",\n",
    "    \"oddly I have no hamburgers\",\n",
    "    \"wish I were here\"\n",
    "]\n",
    "\n",
    "docs = list(map(chunkn, docs))\n",
    "print(docs)\n",
    "\n",
    "bm_crap = BM25Okapi(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.52914208 0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "query = chunkn(\"bob is a big boy now\")\n",
    "scores = bm_crap.get_scores(query)\n",
    "# scores = (scores - np.min(scores))/(np.max(scores)-np.min(scores))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# create index from the cluster \"documents\" which are stored in word_dict\n",
    "#\n",
    "# bm25_index = BM25Okapi(word_dict.values())\n",
    "docs = [chunkn(\" \".join(x)) for x in word_dict.values()]\n",
    "bm25_index = BM25Okapi(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "2 [0.6634944  0.66244606 0.51192153 1.         0.73419402 0.\n",
      " 0.         0.         0.         0.         0.254082   0.\n",
      " 0.26351492 0.         0.         0.         0.         0.\n",
      " 0.        ] 0.5119215320488495\n"
     ]
    }
   ],
   "source": [
    "n=20\n",
    "query = chunkn(test_df.signature.iloc[n])[:25]\n",
    "cluster = test_df.cluster.iloc[n]\n",
    "\n",
    "scores = bm25_index.get_scores(query)\n",
    "print(len(query))\n",
    "scores = (scores - np.min(scores))/(np.max(scores)-np.min(scores))\n",
    "print(cluster,scores, scores[cluster])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1124\n",
      "Sum:1003.9583017866413 Percent:89.32013361091114, Count:1124, Zeros:0\n"
     ]
    }
   ],
   "source": [
    "print(test_df.shape[0])\n",
    "sum=0\n",
    "max = test_df.shape[0]\n",
    "count=0\n",
    "for i in range(test_df.shape[0]):\n",
    "    query = chunkn(test_df.signature.iloc[i])[:100]\n",
    "    cluster = test_df.cluster.iloc[i]\n",
    "    scores = bm25_index.get_scores(query)\n",
    "    if np.max(scores) > np.min(scores):\n",
    "        scores = (scores - np.min(scores))/(np.max(scores)-np.min(scores))\n",
    "        # print(cluster, scores[cluster])\n",
    "        sum += scores[cluster]\n",
    "        count += 1\n",
    "    # else:\n",
    "        # print(cluster,-1)\n",
    "print(f\"Sum:{sum} Percent:{sum/max*100.0}, Count:{count}, Zeros:{max-count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
